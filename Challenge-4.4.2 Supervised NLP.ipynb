{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading gutenberg: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\", line 141, in _new_conn\n",
      "    (self.host, self.port), self.timeout, **extra_kw)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/urllib3/util/connection.py\", line 60, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/socket.py\", line 743, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 601, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 346, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 850, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\", line 284, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\", line 150, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e)\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x11391a2e8>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/requests/adapters.py\", line 440, in send\n",
      "    timeout=timeout\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 639, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/urllib3/util/retry.py\", line 388, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x11391a2e8>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/spacy/__main__.py\", line 35, in <module>\n",
      "    plac.call(commands[command], sys.argv[1:])\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n",
      "    cmd, result = parser.consume(arglist)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/spacy/cli/download.py\", line 37, in download\n",
      "    shortcuts = get_json(about.__shortcuts__, \"available shortcuts\")\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/spacy/cli/download.py\", line 73, in get_json\n",
      "    r = requests.get(url)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/requests/api.py\", line 72, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/requests/api.py\", line 58, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/requests/sessions.py\", line 508, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/requests/sessions.py\", line 618, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/Sri/anaconda3/lib/python3.6/site-packages/requests/adapters.py\", line 508, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x11391a2e8>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n",
    "\n",
    "Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.\n",
    "\n",
    "**Note**: Since processing all the text takes around ~5-10 minutes, in the cell below we are taking only the first tenth of each text. If you want to experiment, feel free to change the following code in the next cell:\n",
    "\n",
    "```python\n",
    "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])\n",
    "```\n",
    "to \n",
    "\n",
    "```python\n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub('(?i)CHAPTER .*', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1411"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(alice)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice was '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sir Walter'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasion[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443, 2)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nay</th>\n",
       "      <th>passage</th>\n",
       "      <th>deputation</th>\n",
       "      <th>inhabit</th>\n",
       "      <th>sense</th>\n",
       "      <th>nation</th>\n",
       "      <th>perfectly</th>\n",
       "      <th>steal</th>\n",
       "      <th>elegance</th>\n",
       "      <th>soothing</th>\n",
       "      <th>...</th>\n",
       "      <th>tired</th>\n",
       "      <th>Hall</th>\n",
       "      <th>whisker</th>\n",
       "      <th>embarrassment</th>\n",
       "      <th>tiny</th>\n",
       "      <th>likely</th>\n",
       "      <th>LOVE</th>\n",
       "      <th>great</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1612 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nay passage deputation inhabit sense nation perfectly steal elegance  \\\n",
       "0   0       0          0       0     0      0         0     0        0   \n",
       "1   0       0          0       0     0      0         0     0        0   \n",
       "2   0       0          0       0     0      0         0     0        0   \n",
       "3   0       0          0       0     0      0         0     0        0   \n",
       "4   0       0          0       0     0      0         0     0        0   \n",
       "\n",
       "  soothing     ...     tired Hall whisker embarrassment tiny likely LOVE  \\\n",
       "0        0     ...         1    0       0             0    0      0    0   \n",
       "1        0     ...         0    0       0             0    0      0    0   \n",
       "2        0     ...         0    0       0             0    0      0    0   \n",
       "3        0     ...         0    0       0             0    0      0    0   \n",
       "4        0     ...         0    0       0             0    0      0    0   \n",
       "\n",
       "  great                                      text_sentence text_source  \n",
       "0     0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1     0  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2     0  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3     0                                      (Oh, dear, !)     Carroll  \n",
       "4     0                                      (Oh, dear, !)     Carroll  \n",
       "\n",
       "[5 rows x 1612 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9773584905660377\n",
      "\n",
      "Test set score: 0.8426966292134831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Holy overfitting, Batman! Overfitting is a known problem when using bag of words, since it basically involves throwing a massive number of features at a model – some of those features (in this case, word frequencies) will capture noise in the training set. Since overfitting is also a known problem with Random Forests, the divergence between training score and test score is expected.\n",
    "\n",
    "\n",
    "## BoW with Logistic Regression\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with ridge regularization (from ridge regression, also called L2 regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265, 1610) (265,)\n",
      "Training set score: 0.969811320754717\n",
      "\n",
      "Test set score: 0.8651685393258427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.969811320754717\n",
      "\n",
      "Test set score: 0.8202247191011236\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Looks like logistic regression is the winner, but there's room for improvement.\n",
    "\n",
    "# Same model, new inputs\n",
    "\n",
    "What if we feed the model a different novel by Jane Austen, like _Emma_?  Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n",
    "\n",
    "First, we need to process _Emma_ the same way we processed the other data, and combine it with the Alice data. Remember that for computation time concerns, we only took the first tenth of the Alice text. Emma is pretty long. **So in order to get comparable length texts, we take the first sixtieth of Emma**. Again, if you want to experiment, you can take the whole texts of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma[:int(len(emma)/60)])\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.6979591836734694\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>158</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen      158       12\n",
       "Carroll      62       13"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n",
    "\n",
    "# Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%.  \n",
    "\n",
    "# Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding phrases to the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "def more_bag_of_words(doc):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    phrases = [chunk.text for chunk in doc.noun_chunks\n",
    "               if (' ' in str(chunk)) and not ('*' in str(chunk))]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(phrases).most_common(2000)]    \n",
    "\n",
    "def more_bag_of_words_ent(doc):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    entities = [chunk.label_ for chunk in doc.ents]\n",
    "    #if (' ' in str(chunk)) and not ('*' in str(chunk))\n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(entities).most_common(2000)]  \n",
    "\n",
    "def more_bag_of_words_pos(doc):\n",
    "    \n",
    "    sentences=doc.sents\n",
    "    \n",
    "    pos = [token.pos_ for sent in sentences for token in sent]\n",
    "    #if (' ' in str(chunk)) and not ('*' in str(chunk))\n",
    "    # Return the most common words.\n",
    "    #return [item[0] for item in Counter(pos).most_common(2000)]  \n",
    "    return pos\n",
    "    \n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "alicephrases = more_bag_of_words(alice_doc)\n",
    "persuasionphrases = more_bag_of_words(persuasion_doc)\n",
    "aliceentities = more_bag_of_words_ent(alice_doc)\n",
    "persuasionentities = more_bag_of_words_ent(persuasion_doc)\n",
    "alicepos = more_bag_of_words_pos(alice_doc)\n",
    "persuasionpos = more_bag_of_words_pos(persuasion_doc)\n",
    "\n",
    "#Finding unique key words\n",
    "\n",
    "# Combine bags to create a set of unique words/pharases\n",
    "common_words = list(set(alicewords + persuasionwords))\n",
    "common_phrases = list(set(alicephrases + persuasionphrases))\n",
    "common_entities = list(set(aliceentities + persuasionentities))\n",
    "common_pos = list(set(alicepos + persuasionpos))\n",
    "unique_words_alice_notinPersuasion=set(alicewords)- set(persuasionwords)\n",
    "unique_words_persuasion_notinAlice=set(persuasionwords)-set(alicewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nay</th>\n",
       "      <th>passage</th>\n",
       "      <th>deputation</th>\n",
       "      <th>inhabit</th>\n",
       "      <th>sense</th>\n",
       "      <th>nation</th>\n",
       "      <th>perfectly</th>\n",
       "      <th>steal</th>\n",
       "      <th>elegance</th>\n",
       "      <th>soothing</th>\n",
       "      <th>...</th>\n",
       "      <th>his first marriage</th>\n",
       "      <th>a much safer place</th>\n",
       "      <th>such the feelings</th>\n",
       "      <th>a toil</th>\n",
       "      <th>our wealthy naval commanders</th>\n",
       "      <th>a VERY good opportunity</th>\n",
       "      <th>a man</th>\n",
       "      <th>a state</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nay passage deputation inhabit sense nation perfectly steal elegance  \\\n",
       "0   0       0          0       0     0      0         0     0        0   \n",
       "1   0       0          0       0     0      0         0     0        0   \n",
       "2   0       0          0       0     0      0         0     0        0   \n",
       "3   0       0          0       0     0      0         0     0        0   \n",
       "4   0       0          0       0     0      0         0     0        0   \n",
       "\n",
       "  soothing     ...     his first marriage a much safer place  \\\n",
       "0        0     ...                      0                  0   \n",
       "1        0     ...                      0                  0   \n",
       "2        0     ...                      0                  0   \n",
       "3        0     ...                      0                  0   \n",
       "4        0     ...                      0                  0   \n",
       "\n",
       "  such the feelings a toil our wealthy naval commanders  \\\n",
       "0                 0      0                            0   \n",
       "1                 0      0                            0   \n",
       "2                 0      0                            0   \n",
       "3                 0      0                            0   \n",
       "4                 0      0                            0   \n",
       "\n",
       "  a VERY good opportunity a man a state  \\\n",
       "0                       0     0       0   \n",
       "1                       0     0       0   \n",
       "2                       0     0       0   \n",
       "3                       0     0       0   \n",
       "4                       0     0       0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3                                      (Oh, dear, !)     Carroll  \n",
       "4                                      (Oh, dear, !)     Carroll  \n",
       "\n",
       "[5 rows x 2787 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences=sentences, common_words=common_words,\n",
    "                 common_entities=common_entities, common_pos=common_pos, common_phrases=common_phrases):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    columns=common_words+common_entities+common_pos+common_phrases\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, columns] = 0\n",
    "     \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        phrases = [ph.text\n",
    "                 for ph in sentence.noun_chunks\n",
    "                 if (' ' in str(ph)) and not ('*' in str(ph)) and ph.text in common_phrases]\n",
    "    \n",
    "    \n",
    "        entities = [e.label_\n",
    "                 for e in sentence.ents\n",
    "                 if (e.label_ in common_entities\n",
    "                 )]\n",
    "        pos = [token.pos_\n",
    "                 for token in sentence\n",
    "                 if (token.pos_ in common_pos\n",
    "                 )]\n",
    "                    # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        #Populate the row with phrase counts.\n",
    "        for phrase in phrases:\n",
    "            df.loc[i, phrase] += 1\n",
    "            \n",
    "        # Populate the row with phrase counts.\n",
    "        for entity in entities:\n",
    "            df.loc[i, entity] += 1\n",
    "            \n",
    "        for token in pos:\n",
    "            df.loc[i, token] += 1\n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "                        \n",
    "    return df\n",
    "\n",
    "word_counts = bow_features()\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 443 entries, 0 to 442\n",
      "Columns: 2787 entries, Nay to text_source\n",
      "dtypes: object(2787)\n",
      "memory usage: 9.4+ MB\n"
     ]
    }
   ],
   "source": [
    "word_counts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = word_counts.drop(['text_sentence','text_source'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(mod, X=X, Y=Y):\n",
    "    #Data split to Train and test\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=1)\n",
    "    mo=mod\n",
    "    mo.fit(X_train, y_train)\n",
    "    print (\"Model Train score is %0.2f\" % (mo.score(X_train, y_train)) )\n",
    "    print (\"Model Test score is %0.2f\" % (mo.score(X_test, y_test)))\n",
    "    return mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train score is 0.98\n",
      "Model Test score is 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model(ensemble.RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train score is 0.98\n",
      "Model Test score is 0.86\n"
     ]
    }
   ],
   "source": [
    "model(ensemble.GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train score is 0.98\n",
      "Model Test score is 0.87\n"
     ]
    }
   ],
   "source": [
    "model(LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train score is 1.00\n",
      "Model Test score is 0.83\n"
     ]
    }
   ],
   "source": [
    "model(SVC(gamma='auto', kernel='linear', C=1e9 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When added common pos, common phrases and  common entities the accuracy increased from 83% to 87% i.e a 4% increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#With just pos alone but a different way of taking in the parts of speech together for each sentence as one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sri/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>dep0</th>\n",
       "      <th>dep1</th>\n",
       "      <th>dep2</th>\n",
       "      <th>dep3</th>\n",
       "      <th>dep4</th>\n",
       "      <th>dep5</th>\n",
       "      <th>dep6</th>\n",
       "      <th>dep7</th>\n",
       "      <th>...</th>\n",
       "      <th>dep433</th>\n",
       "      <th>dep434</th>\n",
       "      <th>dep435</th>\n",
       "      <th>dep436</th>\n",
       "      <th>dep437</th>\n",
       "      <th>dep438</th>\n",
       "      <th>dep439</th>\n",
       "      <th>dep440</th>\n",
       "      <th>dep441</th>\n",
       "      <th>dep442</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 445 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_sentence text_source  dep0  dep1  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll     0     1   \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll     0     0   \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll     0     0   \n",
       "3                                      (Oh, dear, !)     Carroll     0     0   \n",
       "4                                      (Oh, dear, !)     Carroll     0     0   \n",
       "\n",
       "   dep2  dep3  dep4  dep5  dep6  dep7   ...    dep433  dep434  dep435  dep436  \\\n",
       "0     0     0     0     0     0     0   ...         0       0       0       0   \n",
       "1     1     0     0     0     0     0   ...         0       0       0       0   \n",
       "2     0     1     0     0     0     0   ...         0       0       0       0   \n",
       "3     0     0     1     0     0     0   ...         0       0       0       0   \n",
       "4     0     0     0     1     0     0   ...         0       0       0       0   \n",
       "\n",
       "   dep437  dep438  dep439  dep440  dep441  dep442  \n",
       "0       0       0       0       0       0       0  \n",
       "1       0       0       0       0       0       0  \n",
       "2       0       0       0       0       0       0  \n",
       "3       0       0       0       0       0       0  \n",
       "4       0       0       0       0       0       0  \n",
       "\n",
       "[5 rows x 445 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences=sentences):\n",
    "    df=pd.DataFrame()\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "       \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "      \n",
    "        #pos = [token.pos_ for token in sentence]\n",
    "        \n",
    "        dep = [token.dep_ for token in sentence]\n",
    "        \n",
    "        #df['pos'+ str(i)]=0\n",
    "        #df['pos'+ str(i)][(i-1):i] = 1\n",
    "        \n",
    "        df['dep'+ str(i)]=0\n",
    "        df['dep'+ str(i)][(i-1):i] = 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "                        \n",
    "    return df\n",
    "\n",
    "spacy_df = bow_features()\n",
    "spacy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = spacy_df['text_source']\n",
    "X = spacy_df.drop(['text_sentence','text_source'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train score is 0.98\n",
      "Model Test score is 0.87\n"
     ]
    }
   ],
   "source": [
    "model(LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With just pos or dep or combined the result is 87% i.e alone they can equal all the other features combined in accuracy. \n",
    "\n",
    "This is very interesting phenomenon, which needs to discussed and analysed in detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different approach one can take is to see if taking unique key words in each text alice and persuasion and see that is predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords)]\n",
    "    \n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "#Finding unique key words\n",
    "unique_words_alice_notinPersuasion=set(alicewords)- set(persuasionwords)\n",
    "unique_words_persuasion_notinAlice=set(persuasionwords)-set(alicewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sri/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Sri/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>2</th>\n",
       "      <th>K</th>\n",
       "      <th>T</th>\n",
       "      <th>`</th>\n",
       "      <th>I</th>\n",
       "      <th>G</th>\n",
       "      <th>z</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>pos438</th>\n",
       "      <th>dep438</th>\n",
       "      <th>pos439</th>\n",
       "      <th>dep439</th>\n",
       "      <th>pos440</th>\n",
       "      <th>dep440</th>\n",
       "      <th>pos441</th>\n",
       "      <th>dep441</th>\n",
       "      <th>pos442</th>\n",
       "      <th>dep442</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q  S  2  K  T  `  I  G  z  9   ...   pos438 dep438 pos439 dep439 pos440  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0   ...        0      0      0      0      0   \n",
       "1  0  0  0  0  0  0  0  0  0  0   ...        0      0      0      0      0   \n",
       "2  0  0  0  0  0  0  0  0  0  0   ...        0      0      0      0      0   \n",
       "3  0  0  0  0  0  0  0  0  0  0   ...        0      0      0      0      0   \n",
       "4  0  0  0  0  0  0  0  0  0  0   ...        0      0      0      0      0   \n",
       "\n",
       "  dep440 pos441 dep441 pos442  dep442  \n",
       "0      0      0      0      0       0  \n",
       "1      0      0      0      0       0  \n",
       "2      0      0      0      0       0  \n",
       "3      0      0      0      0       0  \n",
       "4      0      0      0      0       0  \n",
       "\n",
       "[5 rows x 905 columns]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences=sentences,\n",
    "                 unique_words_alice_notinPersuasion=unique_words_alice_notinPersuasion,\n",
    "                unique_words_persuasion_notinAlice=unique_words_persuasion_notinAlice):\n",
    "    columns=list(unique_words_persuasion_notinAlice)+list(unique_words_alice_notinPersuasion)\n",
    "    df=pd.DataFrame(columns=columns)\n",
    "    \n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, columns] = 0\n",
    "       \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        words = [token.lemma_ for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in (unique_words_persuasion_notinAlice\n",
    "                     or unique_words_alice_notinPersuasion)\n",
    "                 )]\n",
    "        \n",
    "        for word in words:\n",
    "                df.loc[i, word] += 1\n",
    "       \n",
    "       \n",
    "        pos = [token.pos_ for token in sentence]\n",
    "        \n",
    "        dep = [token.dep_ for token in sentence]\n",
    "        \n",
    "        df['pos'+ str(i)]=0\n",
    "        df['pos'+ str(i)][i:(i+1)] = 1\n",
    "        \n",
    "        df['dep'+ str(i)]=0\n",
    "        df['dep'+ str(i)][i:(i+1)] = 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "                        \n",
    "    return df\n",
    "\n",
    "spacy_df = bow_features()\n",
    "spacy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = spacy_df['text_source']\n",
    "X = spacy_df.drop(['text_sentence','text_source'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train score is 0.99\n",
      "Model Test score is 0.88\n"
     ]
    }
   ],
   "source": [
    "mo=model(LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train score is 0.98\n",
      "Model Test score is 0.86\n"
     ]
    }
   ],
   "source": [
    "model(ensemble.GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that with using unique key words alone or in combination with other pos and dep all give 88% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next attempt is to combine al the features so far done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "def more_bag_of_words(doc):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    phrases = [chunk.text for chunk in doc.noun_chunks\n",
    "               if (' ' in str(chunk)) and not ('*' in str(chunk))]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(phrases).most_common(2000)]    \n",
    "\n",
    "def more_bag_of_words_ent(doc):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    entities = [chunk.label_ for chunk in doc.ents]\n",
    "    #if (' ' in str(chunk)) and not ('*' in str(chunk))\n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(entities).most_common(2000)]  \n",
    "\n",
    "def more_bag_of_words_pos(doc):\n",
    "    \n",
    "    sentences=doc.sents\n",
    "    \n",
    "    pos = [token.pos_ for sent in sentences for token in sent]\n",
    "    #if (' ' in str(chunk)) and not ('*' in str(chunk))\n",
    "    # Return the most common words.\n",
    "    #return [item[0] for item in Counter(pos).most_common(2000)]  \n",
    "    return pos\n",
    "    \n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "alicephrases = more_bag_of_words(alice_doc)\n",
    "persuasionphrases = more_bag_of_words(persuasion_doc)\n",
    "aliceentities = more_bag_of_words_ent(alice_doc)\n",
    "persuasionentities = more_bag_of_words_ent(persuasion_doc)\n",
    "alicepos = more_bag_of_words_pos(alice_doc)\n",
    "persuasionpos = more_bag_of_words_pos(persuasion_doc)\n",
    "\n",
    "#Finding unique key words\n",
    "\n",
    "# Combine bags to create a set of unique words/pharases\n",
    "common_words = list(set(alicewords + persuasionwords))\n",
    "common_phrases = list(set(alicephrases + persuasionphrases))\n",
    "common_entities = list(set(aliceentities + persuasionentities))\n",
    "common_pos = list(set(alicepos + persuasionpos))\n",
    "unique_words_alice_notinPersuasion=set(alicewords)- set(persuasionwords)\n",
    "unique_words_persuasion_notinAlice=set(persuasionwords)-set(alicewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nay</th>\n",
       "      <th>passage</th>\n",
       "      <th>deputation</th>\n",
       "      <th>inhabit</th>\n",
       "      <th>sense</th>\n",
       "      <th>nation</th>\n",
       "      <th>perfectly</th>\n",
       "      <th>steal</th>\n",
       "      <th>elegance</th>\n",
       "      <th>soothing</th>\n",
       "      <th>...</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>economy</th>\n",
       "      <th>embarrassment</th>\n",
       "      <th>compassion</th>\n",
       "      <th>scarcely</th>\n",
       "      <th>preside</th>\n",
       "      <th>well</th>\n",
       "      <th>Russell</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nay passage deputation inhabit sense nation perfectly steal elegance  \\\n",
       "0   0       0          0       0     0      0         0     0        0   \n",
       "1   0       0          0       0     0      0         0     0        0   \n",
       "2   0       0          0       0     0      0         0     0        0   \n",
       "3   0       0          0       0     0      0         0     0        0   \n",
       "4   0       0          0       0     0      0         0     0        0   \n",
       "\n",
       "  soothing     ...     neighbourhood economy embarrassment compassion  \\\n",
       "0        0     ...                 0       0             0          0   \n",
       "1        0     ...                 0       0             0          0   \n",
       "2        0     ...                 0       0             0          0   \n",
       "3        0     ...                 0       0             0          0   \n",
       "4        0     ...                 0       0             0          0   \n",
       "\n",
       "  scarcely preside well Russell  \\\n",
       "0        0       0    0       0   \n",
       "1        0       0    0       0   \n",
       "2        0       0    0       0   \n",
       "3        0       0    0       0   \n",
       "4        0       0    0       0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3                                      (Oh, dear, !)     Carroll  \n",
       "4                                      (Oh, dear, !)     Carroll  \n",
       "\n",
       "[5 rows x 4204 columns]"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences=sentences, common_words=common_words,\n",
    "                 common_entities=common_entities, common_pos=common_pos, common_phrases=common_phrases,\n",
    "                unique_words_alice_notinPersuasion=unique_words_alice_notinPersuasion,\n",
    "                unique_words_persuasion_notinAlice=unique_words_persuasion_notinAlice):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    columns=common_words+common_entities+common_pos+common_phrases+list(unique_words_alice_notinPersuasion)+list(unique_words_persuasion_notinAlice)\n",
    "    \n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, columns] = 0\n",
    "     \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        uniq_words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in (unique_words_persuasion_notinAlice\n",
    "                     or unique_words_alice_notinPersuasion))]\n",
    "        phrases = [ph.text\n",
    "                 for ph in sentence.noun_chunks\n",
    "                 if (' ' in str(ph)) and not ('*' in str(ph)) and ph.text in common_phrases]\n",
    "    \n",
    "    \n",
    "        entities = [e.label_\n",
    "                 for e in sentence.ents\n",
    "                 if (e.label_ in common_entities\n",
    "                 )]\n",
    "        pos = [token.pos_\n",
    "                 for token in sentence\n",
    "                 if (token.pos_ in common_pos\n",
    "                 )]\n",
    "                    # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # Populate the row with phrase counts.\n",
    "        for phrase in phrases:\n",
    "            df.loc[i, phrase] += 1\n",
    "            \n",
    "        # Populate the row with phrase counts.\n",
    "        for entity in entities:\n",
    "            df.loc[i, entity] += 1\n",
    "            \n",
    "        for token in pos:\n",
    "            df.loc[i, token] += 1\n",
    "        for word in uniq_words:\n",
    "            df.loc[i, word] += 1\n",
    "     \n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "        \n",
    "    return df\n",
    "\n",
    "word_counts = bow_features()\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = word_counts.drop(['text_sentence','text_source'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train score is 0.99\n",
      "Model Test score is 0.88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "With all the features the accuracy acheivable till now is 87%. Let us how we can apply this model to other texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near the ruins of the castle of Rossmore, in Ireland, is a small cabin, in which there once lived a \n"
     ]
    }
   ],
   "source": [
    "# Clean the Edgeworth-parents data.\n",
    "edge = gutenberg.raw('edgeworth-parents.txt')\n",
    "edge = re.sub('[A-Z]* [A-Z]*\\.', '', edge)\n",
    "edge = text_cleaner(edge[:int(len(alice))])\n",
    "print(edge[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "edge_doc = nlp(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "edge_sents = [[sent, \"Edgeworth\"] for sent in edge_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239, 2)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Edge word count\n",
    "sentences = pd.DataFrame(alice_sents+edge_sents)\n",
    "sentences.head()\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "def more_bag_of_words(doc):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    phrases = [chunk.text for chunk in doc.noun_chunks\n",
    "               if (' ' in str(chunk)) and not ('*' in str(chunk))]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(phrases).most_common(2000)]    \n",
    "\n",
    "def more_bag_of_words_ent(doc):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    entities = [chunk.label_ for chunk in doc.ents]\n",
    "    #if (' ' in str(chunk)) and not ('*' in str(chunk))\n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(entities).most_common(2000)]  \n",
    "\n",
    "def more_bag_of_words_pos(doc):\n",
    "    \n",
    "    sentences=doc.sents\n",
    "    \n",
    "    pos = [token.pos_ for sent in sentences for token in sent]\n",
    "    #if (' ' in str(chunk)) and not ('*' in str(chunk))\n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(pos).most_common(2000)]  \n",
    "\n",
    "    \n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "edgewords = bag_of_words(edge_doc)\n",
    "alicephrases = more_bag_of_words(alice_doc)\n",
    "edgephrases = more_bag_of_words(edge_doc)\n",
    "aliceentities = more_bag_of_words_ent(alice_doc)\n",
    "edgeentities = more_bag_of_words_ent(edge_doc)\n",
    "alicepos = more_bag_of_words_pos(alice_doc)\n",
    "edgepos = more_bag_of_words_pos(edge_doc)\n",
    "\n",
    "#Finding unique key words\n",
    "\n",
    "# Combine bags to create a set of unique words/pharases\n",
    "common_words = list(set(alicewords + edgewords))\n",
    "common_phrases = list(set(alicephrases + edgephrases))\n",
    "common_entities = list(set(aliceentities + edgeentities))\n",
    "common_pos = list(set(alicepos + edgepos))\n",
    "unique_words_alice_notinEdge=set(alicewords)- set(edgewords)\n",
    "unique_words_Edge_notinAlice=set(edgewords)-set(alicewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dear</th>\n",
       "      <th>mind</th>\n",
       "      <th>grow</th>\n",
       "      <th>hint</th>\n",
       "      <th>passage</th>\n",
       "      <th>seek</th>\n",
       "      <th>large</th>\n",
       "      <th>bethink</th>\n",
       "      <th>employ</th>\n",
       "      <th>stock</th>\n",
       "      <th>...</th>\n",
       "      <th>fourpence</th>\n",
       "      <th>rush</th>\n",
       "      <th>softly</th>\n",
       "      <th>nature</th>\n",
       "      <th>bleach</th>\n",
       "      <th>eld</th>\n",
       "      <th>fine</th>\n",
       "      <th>guinea</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1891 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dear mind grow hint passage seek large bethink employ stock     ...      \\\n",
       "0    0    0    0    0       0    0     0       0      0     0     ...       \n",
       "1    0    1    0    0       0    0     0       0      0     0     ...       \n",
       "2    0    0    0    0       0    0     0       0      0     0     ...       \n",
       "3    1    0    0    0       0    0     0       0      0     0     ...       \n",
       "4    1    0    0    0       0    0     0       0      0     0     ...       \n",
       "\n",
       "  fourpence rush softly nature bleach eld fine guinea  \\\n",
       "0         0    0      0      0      0   0    0      0   \n",
       "1         0    0      0      0      0   0    0      0   \n",
       "2         0    0      0      0      0   0    0      0   \n",
       "3         0    0      0      0      0   0    0      0   \n",
       "4         0    0      0      0      0   0    0      0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3                                      (Oh, dear, !)     Carroll  \n",
       "4                                      (Oh, dear, !)     Carroll  \n",
       "\n",
       "[5 rows x 1891 columns]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences=sentences, common_words=common_words,\n",
    "                 common_entities=common_entities, common_pos=common_pos, common_phrases=common_phrases,\n",
    "                unique_words_alice_notinEdge=unique_words_alice_notinEdge,\n",
    "                unique_words_Edge_notinAlice=unique_words_Edge_notinAlice):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    columns=common_words+common_entities+common_pos+common_phrases+list(unique_words_alice_notinEdge)+list(unique_words_Edge_notinAlice)\n",
    "    \n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, columns] = 0\n",
    "     \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        uniq_words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in (unique_words_Edge_notinAlice\n",
    "                     or unique_words_alice_notinEdge))]\n",
    "        phrases = [ph.text\n",
    "                 for ph in sentence.noun_chunks\n",
    "                 if (' ' in str(ph)) and not ('*' in str(ph)) and ph.text in common_phrases]\n",
    "    \n",
    "    \n",
    "        entities = [e.label_\n",
    "                 for e in sentence.ents\n",
    "                 if (e.label_ in common_entities\n",
    "                 )]\n",
    "        pos = [token.pos_\n",
    "                 for token in sentence\n",
    "                 if (token.pos_ in common_pos\n",
    "                 )]\n",
    "                    # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # Populate the row with phrase counts.\n",
    "        for phrase in phrases:\n",
    "            df.loc[i, phrase] += 1\n",
    "            \n",
    "        # Populate the row with phrase counts.\n",
    "        for entity in entities:\n",
    "            df.loc[i, entity] += 1\n",
    "            \n",
    "        for token in pos:\n",
    "            df.loc[i, token] += 1\n",
    "        for word in uniq_words:\n",
    "            df.loc[i, word] += 1\n",
    "        pos = [token.pos_ for token in sentence]\n",
    "        \n",
    "        dep = [token.dep_ for token in sentence]\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "        \n",
    "    return df\n",
    "\n",
    "word_counts = bow_features()\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = word_counts.drop(['text_sentence','text_source'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train score is 0.99\n",
      "Model Test score is 0.88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does well with 88%(same as got before) when used to train another text document edge along with alice txt. It has got the same accuracy as alice and persuasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
